{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make sure we have all the races"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('./../data/results/*')\n",
    "filenames = list(map(lambda x: x.split('/')[-1], files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2011 already present for race portmacquarie70.3\n",
      "Year 2008 already present for race arizona\n",
      "Data for 199 races have been downloaded.\n"
     ]
    }
   ],
   "source": [
    "results_downloaded = {}\n",
    "\n",
    "for name in filenames:\n",
    "    race,year,month,day = re.match(\"(.+)_(\\d{4})(\\d{2})(\\d{2}).jl\", name).groups()\n",
    "    race = race.lower()\n",
    "    if race not in results_downloaded:\n",
    "        results_downloaded[race] = {\n",
    "            'years': [year],\n",
    "            'dates': [f'{year}{month}{day}']\n",
    "        }\n",
    "    else:\n",
    "        if year in results_downloaded[race]['years']:\n",
    "            print(f'Year {year} already present for race {race}')\n",
    "        results_downloaded[race]['years'].append(year)\n",
    "        results_downloaded[race]['dates'].append(f'{year}{month}{day}')\n",
    "    \n",
    "print(f'Data for {len(results_downloaded)} races have been downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from races listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A listing of 211 races had been inputed\n"
     ]
    }
   ],
   "source": [
    "all_races = {}\n",
    "with open('./../data/races/races.jl', 'r') as f:\n",
    "    data = [json.loads(line.strip()) for line in f.readlines()]\n",
    "for race in data:\n",
    "    if race:\n",
    "        all_races[race['id'].lower()] = {'id': race['id'], 'name': race['name'], 'website': race['website']}\n",
    "        \n",
    "print(f'A listing of {len(all_races)} races had been inputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = {}\n",
    "for race in all_races:\n",
    "    if all_races[race]['id'].lower() in ids:\n",
    "        print(f'race id {all_races[race][\"id\"]} already present')\n",
    "    else:\n",
    "        ids[all_races[race][\"id\"].lower()] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get missing races info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffalosprings70.3 => http://www.ironman.com/triathlon/events/americas/ironman-70.3/inactive/buffalo-springs-lake.aspx\n",
      "traversecity70.3 => http://www.ironman.com/triathlon/events/americas/ironman-70.3/traverse-city.aspx\n",
      "jeju70.3 => http://www.ironman.com/triathlon/events/asiapac/ironman-70.3/goseong-korea.aspx\n",
      "stcroix70.3 => http://www.ironman.com/triathlon/events/americas/ironman-70.3/inactive/st.-croix.aspx\n",
      "hefei70.3 => http://www.ironman.com/triathlon/events/asiapac/ironman-70.3/hefei.aspx\n",
      "korea70.3 => http://www.ironman.com/triathlon/events/asiapac/ironman-70.3/gurye-korea.aspx\n",
      "chungju70.3 => http://www.ironman.com/triathlon/events/asiapac/ironman-70.3/inactive/chungju-korea.aspx\n",
      "Vineman => http://www.ironman.com/triathlon/events/americas/ironman/vineman.aspx\n",
      "busan70.3 => http://www.ironman.com/triathlon/events/asiapac/ironman-70.3/inactive/busan-korea.aspx\n",
      "putrajaya70.3 => http://www.ironman.com/triathlon/events/asiapac/ironman-70.3/inactive/putrajaya.aspx\n",
      "ireland70.3 => http://www.ironman.com/triathlon/events/emea/ironman-70.3/inactive/galway.aspx\n",
      "monaco70.3 => http://www.ironman.com/triathlon/events/emea/ironman-70.3/inactive/monaco.aspx\n",
      "12 race results are missing\n"
     ]
    }
   ],
   "source": [
    "missing_results = []\n",
    "for race in all_races:\n",
    "    if not results_downloaded.get(race):\n",
    "        missing_results.append(all_races[race])\n",
    "        print(f\"{all_races[race]['id']} => {all_races[race]['website']}\")\n",
    "print(f'{len(missing_results)} race results are missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file for download\n",
    "with open('missing_races.jl', 'w') as f:\n",
    "    for race in missing_results:\n",
    "        f.write(f'{json.dumps(race)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if correct count has been downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 1071 races have been checked\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('./../data/results/*')\n",
    "filenames = list(map(lambda x: x.split('/')[-1], files))\n",
    "\n",
    "counts_downloaded = {}\n",
    "\n",
    "for i,name in enumerate(filenames):\n",
    "    race,year,month,day = re.match(\"(.+)_(\\d{4})(\\d{2})(\\d{2}).jl\", name).groups()\n",
    "    race = race.lower()\n",
    "    race_id = f\"{race}_{year}{month}{day}\"\n",
    "    if race_id not in counts_downloaded:\n",
    "        with open(files[i], 'r') as f:\n",
    "            counts_downloaded[race_id] = len(f.readlines(  ))\n",
    "    else:\n",
    "        print(f\"race {race} ({year}{month}{day}) is already present\")\n",
    "    \n",
    "print(f'Data for {len(counts_downloaded)} races have been checked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare with scraped counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../data/races/races-athletes-count.jl', 'r') as f:\n",
    "    data_count = [json.loads(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "counts_scraped = {}\n",
    "for race in data_count:\n",
    "    race_id = f\"{race['id'].lower()}_{race['date']}\"\n",
    "    # only take in account the race if there are actually results\n",
    "    if race['count']>10:\n",
    "        counts_scraped[race_id] = race['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australia_20170507 1026 (1132)\n",
      "australia_20150503 1530 (1650)\n",
      "australia_20180506 1108 (1206)\n",
      "australia_20190505 1309 (1401)\n",
      "australia_20160501 1249 (1355)\n",
      "westernaustralia_20151206 1187 (1203)\n",
      "westernaustralia_20161204 1365 (1382)\n",
      "westernaustralia_20171203 1440 (1461)\n",
      "westernaustralia_20181202 1120 (1146)\n",
      "coquimbo70.3_20171021 389 (457)\n",
      "uk_20130804 1601 (1602)\n",
      "lakeplacid_20050724 2207 (2208)\n",
      "miami70.3_20141026 2927 (3137)\n",
      "branson70.3_20100919 1356 (1357)\n",
      "regensburg_20120617 1292 (1322)\n",
      "branson70.3_20120923 859 (860)\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "not_none = set()\n",
    "incomplete = {}\n",
    "for count in counts_scraped:\n",
    "    scraped = counts_scraped[count]\n",
    "    downloaded = counts_downloaded.get(count)\n",
    "    if scraped != downloaded:\n",
    "        n +=1\n",
    "        print(count, downloaded, f'({scraped})')\n",
    "        if downloaded:\n",
    "            not_none.add(count.split('_')[0])\n",
    "            if not incomplete.get(count.split('_')[0]):\n",
    "                incomplete[count.split('_')[0]] = [count.split('_')[1]]\n",
    "            else:\n",
    "                incomplete[count.split('_')[0]].append(count.split('_')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('not-complete.txt', 'w') as f:\n",
    "    for race in not_none:\n",
    "        f.write(f'\"{race}\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('incomplete.jl', 'w') as f:\n",
    "    f.write(json.dumps(incomplete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'taiwan70.3': ['20170319', '20180318', '20190324'],\n",
       " 'newzealand': ['20140301', '20160305', '20170304', '20180303', '20190302'],\n",
       " 'geelong70.3': ['20170219'],\n",
       " 'davao70.3': ['20190324', '20180325'],\n",
       " 'monterrey70.3': ['20180513'],\n",
       " 'liuzhou70.3': ['20180414'],\n",
       " 'florida70.3': ['20130519'],\n",
       " 'australia': ['20170507',\n",
       "  '20150503',\n",
       "  '20110501',\n",
       "  '20180506',\n",
       "  '20190505',\n",
       "  '20160501'],\n",
       " 'mallorca70.3': ['20150509'],\n",
       " 'vietnam70.3': ['20170507', '20180513'],\n",
       " 'florida': ['20031108'],\n",
       " 'malaysia': ['20161112'],\n",
       " 'loscabos70.3': ['20161030', '20171112'],\n",
       " 'xiamen70.3': ['20171112'],\n",
       " 'sydney70.3': ['20171126'],\n",
       " 'westernaustralia': ['20151206',\n",
       "  '20161204',\n",
       "  '20171203',\n",
       "  '20181202',\n",
       "  '20121209'],\n",
       " 'atlanticcity70.3': ['20160918', '20170917', '20180923'],\n",
       " 'worldchampionship': ['20181013'],\n",
       " 'coquimbo70.3': ['20171021'],\n",
       " 'neworleans70.3': ['20130421', '20150419', '20110417', '20140413'],\n",
       " 'shanghai70.3': ['20181021'],\n",
       " 'augusta70.3': ['20090927'],\n",
       " 'sunshinecoast70.3': ['20170910', '20180826'],\n",
       " 'superfrog70.3': ['20140928'],\n",
       " 'taiwan': ['20171001', '20181007'],\n",
       " 'copenhagen': ['20130818', '20180819'],\n",
       " 'maine70.3': ['20170827'],\n",
       " 'gdynia70.3': ['20170806'],\n",
       " 'philippines70.3': ['20100822',\n",
       "  '20110814',\n",
       "  '20130804',\n",
       "  '20140803',\n",
       "  '20150802',\n",
       "  '20180805',\n",
       "  '20170806'],\n",
       " 'qujing70.3': ['20180805'],\n",
       " 'steelhead70.3': ['20100731', '20110814'],\n",
       " 'worldchampionship70.3m': ['20180902'],\n",
       " 'salzburg70.3': ['20180826'],\n",
       " 'cancun70.3': ['20150920'],\n",
       " 'monttremblant70.3': ['20180624'],\n",
       " 'uk': ['20130804'],\n",
       " 'switzerland': ['20110710', '20140727'],\n",
       " 'santarosa70.3': ['20170513'],\n",
       " 'canada70.3': ['20180729'],\n",
       " 'subicbay70.3': ['20150308', '20160306', '20170312'],\n",
       " 'brazil': ['20150531', '20180527'],\n",
       " 'lakeplacid': ['20050724'],\n",
       " 'kronborg70.3': ['20160619', '20170618'],\n",
       " 'wisconsin70.3': ['20170611'],\n",
       " 'raleigh70.3': ['20150531'],\n",
       " 'switzerland70.3': ['20120603'],\n",
       " 'honu70.3': ['20170603'],\n",
       " 'luxembourg70.3': ['20170618'],\n",
       " 'italy70.3': ['20140601'],\n",
       " 'loscabos': ['20130317'],\n",
       " 'miami70.3': ['20141026'],\n",
       " 'longhorn70.3': ['20131027'],\n",
       " 'pula70.3': ['20170917'],\n",
       " 'austria70.3': ['20080524', '20100530', '20130526', '20140525', '20180527'],\n",
       " 'eagleman70.3': ['20100613', '20130609', '20120610', '20180610'],\n",
       " 'northcarolina': ['20161022'],\n",
       " 'timberman70.3': ['20150816'],\n",
       " 'ballarat70.3': ['20161211'],\n",
       " 'mandura70.3': ['20121021'],\n",
       " 'lakestevens70.3': ['20120715', '20090816'],\n",
       " 'incheon70.3': ['20150920'],\n",
       " 'budapest70.3': ['20160730'],\n",
       " 'boise70.3': ['20080601'],\n",
       " 'branson70.3': ['20100919', '20120923'],\n",
       " 'singapore70.3': ['20120318'],\n",
       " 'regensburg': ['20120617'],\n",
       " 'kansas70.3': ['20090614'],\n",
       " 'auckland70.3': ['20150118']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_races_urls(file, selection=None):\n",
    "    # read file created by racespider\n",
    "    urls = set()\n",
    "    races = []\n",
    "    with open(file, 'r') as f:\n",
    "        data = [json.loads(line.strip()) for line in f.readlines()]\n",
    "    for race in data:\n",
    "        if race:\n",
    "            if selection:\n",
    "                if not any(race_id.lower() in race['id'].lower() for race_id in map(lambda x: x['id'], selection)):\n",
    "                    continue\n",
    "            root = re.match('(.*).asp', race['website']).group(1)\n",
    "            race_results_url = f\"{root}/results.aspx\"\n",
    "            if race_results_url not in urls:\n",
    "                races.append({'id': race['id'], 'url': race_results_url, 'region': race['region']})\n",
    "                urls.add(race_results_url)\n",
    "    if not selection:\n",
    "        # add world championship 70.3 (to also get female results)\n",
    "        races.append({'id': 'worldchampionship70.3', 'region': 'americas' , 'url': 'http://www.ironman.com/triathlon/events/americas/ironman-70.3/70.3-world-championship-womens-race/results.aspx'})\n",
    "    return races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'france70.3',\n",
       "  'url': 'http://www.ironman.com/triathlon/events/emea/ironman-70.3/pays-d-aix/results.aspx',\n",
       "  'region': 'emea'}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection = [\n",
    "  {\n",
    "    \"id\": 'France70.3',\n",
    "    \"years\": []\n",
    "  }\n",
    "]\n",
    "get_races_urls('./../data/races/races.jl', selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'France70.3', 'years': []}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x[\"id\"] == \"France70.3\" , selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data-science)",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
